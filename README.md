# cs182
cs182 sp21 implementation

HW 1
### Q1: Fully-connected Neural Network (35 points)  DONE
The IPython notebook `FullyConnectedNets.ipynb` will introduce you to our
modular layer design, and then use those layers to implement fully-connected
networks of arbitrary depth. To optimize these models you will implement several
popular update rules.

### Q2: Batch Normalization (25 points)        DONE
In the IPython notebook `BatchNormalization.ipynb` you will implement batch
normalization, and use it to train deep fully-connected networks.

### Q3: Dropout (10 points)       DONE
The IPython notebook `Dropout.ipynb` will help you implement Dropout and explore
its effects on model generalization.

### Q4: ConvNet (20 points)       NOT YET
In the IPython Notebook `ConvolutionalNetworks.ipynb` you will implement several
new layers that are commonly used in convolutional networks as well as implement
a small convolutional network.


### Q5: Train a model on CIFAR10 using Pytorch! (10 points)       NOT YET
Now that you've implemented and gained an understanding for many key components 
of a basic deep learning library, it is time to move on to a modern deep learning
library: Pytorch. Here, we will walk you through the key concepts of PyTorch, and
you will use it to experiment and train a model on CIFAR10. We highly recommend 
you use Google Colab (https://colab.research.google.com/) for this notebook, as it
comes with Pytorch installed and provides access to GPUs.


HW 3
### Q1: Language Modeling  DONE



### Q2: Summarization  (Transformer)        DONE


### Q3: Distilation       NOT YET

